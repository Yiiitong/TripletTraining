# This file is part of From Barlow Twins to Triplet Training: Differentiating Dementia with Limited Data (Triplet Training).
#
# Triplet Training is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Triplet Training is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Triplet Training. If not, see <https://www.gnu.org/licenses/>.

import logging
import torch
import math
from torch import nn
import torchvision.models as models
import torch.nn.functional as F
from torch.nn.utils.weight_norm import WeightNorm
from torch.autograd import Function
import torch.distributed as dist
import random

import numpy as np
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

LOG = logging.getLogger(__name__)

class BarlowTwinsLoss(nn.Module):
    def __init__(self, batch_size, lambda_coeff=5e-3, z_dim=128):
        super().__init__()

        self.z_dim = z_dim
        self.batch_size = batch_size
        self.lambda_coeff = lambda_coeff

    def off_diagonal_ele(self, x):
        # taken from: https://github.com/facebookresearch/barlowtwins/blob/main/main.py
        # return a flattened view of the off-diagonal elements of a square matrix
        n, m = x.shape
        assert n == m
        return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()

    def forward(self, z1, z2):
        # N x D, where N is the batch size and D is output dim of projection head
        # comment because the normalization has been done in the projector head
        # z1_norm = (z1 - torch.mean(z1, dim=0)) / torch.std(z1, dim=0)
        # z2_norm = (z2 - torch.mean(z2, dim=0)) / torch.std(z2, dim=0)
        # cross_corr = torch.matmul(z1_norm.T, z2_norm) / self.batch_size

        self.batch_size = z1.shape[0]
        cross_corr = torch.matmul(z1.T, z2) / self.batch_size

        on_diag = torch.diagonal(cross_corr).add_(-1).pow_(2).sum()
        off_diag = self.off_diagonal_ele(cross_corr).pow_(2).sum()

        return (on_diag + self.lambda_coeff * off_diag)

class VICRegLoss(nn.Module):
    def __init__(self, batch_size, lambda_in=25.0, lambda_va=25.0, lambda_co=1.0, z_dim=1024):
        super().__init__()

        self.z_dim = z_dim
        self.batch_size = batch_size
        self.lambda_in = lambda_in
        self.lambda_va = lambda_va
        self.lambda_co = lambda_co

    def off_diagonal(self, x):
        # taken from: https://github.com/facebookresearch/barlowtwins/blob/main/main.py
        # return a flattened view of the off-diagonal elements of a square matrix
        n, m = x.shape
        assert n == m
        return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()
    
    def forward(self, z1, z2):
        # N x D, where N is the batch size and D is output dim of projection head

        self.batch_size = z1.shape[0]

        repr_loss = F.mse_loss(z1, z2)

        z1 = z1 - z1.mean(dim=0)
        z2 = z2 - z2.mean(dim=0)

        std_z1 = torch.sqrt(z1.var(dim=0) + 0.0001)
        std_z2 = torch.sqrt(z2.var(dim=0) + 0.0001)
        std_loss = torch.mean(F.relu(1 - std_z1)) / 2 + torch.mean(F.relu(1 - std_z2)) / 2

        cov_z1 = (z1.T @ z1) / (self.batch_size - 1)
        cov_z2 = (z2.T @ z2) / (self.batch_size - 1)
        cov_loss = self.off_diagonal(cov_z1).pow_(2).sum().div(
            self.z_dim) + self.off_diagonal(cov_z2).pow_(2).sum().div(self.z_dim)

        loss = (
            self.lambda_in * repr_loss
            + self.lambda_va * std_loss
            + self.lambda_co * cov_loss
        )

        return loss, repr_loss, std_loss, cov_loss

class ProjectionHead(nn.Module):
    def __init__(self, num_layer=3, input_dim=2048, hidden_dim=2048, output_dim=128, last_bn=True):
        super().__init__()

        self.last_bn = last_bn
        # change projection head to multiple layers
        layers = []
        layers.append(nn.Linear(input_dim, hidden_dim, bias=False))
        layers.append(nn.BatchNorm1d(hidden_dim))
        layers.append(nn.ReLU(inplace=True))
        num_layer -= 1
        next_dim = hidden_dim
        next_out_dim = hidden_dim
        if num_layer > 1:
            for i in range(num_layer-1):
                next_out_dim = next_dim * (i+1)
                layers.append(nn.Linear(next_dim, next_out_dim, bias=False))
                layers.append(nn.BatchNorm1d(next_out_dim))
                layers.append(nn.ReLU(inplace=True))
                next_dim = next_out_dim
        
        layers.append(nn.Linear(next_out_dim, output_dim, bias=False))
        
        self.projection_head = nn.Sequential(*layers)
            
        # normalization layer for the representations z1 and z2
        self.bn = nn.BatchNorm1d(output_dim, affine=False)


    def forward(self, x):
        out = self.projection_head(x)
        if self.last_bn:
            out = self.bn(out)
        return out

class ClassificationHead(nn.Module):
    def __init__(self, input_dim=2048, hidden_dim=2048, output_dim=3):
        super().__init__()

        self.classification_head = nn.Sequential(
            nn.Linear(input_dim, hidden_dim, bias=False),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, output_dim),
        )

    def forward(self, x):
        out = self.classification_head(x)
        return out

class SupConLoss(nn.Module):
    """Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.
    It also supports the unsupervised contrastive loss in SimCLR"""
    def __init__(self, temperature=0.5, contrast_mode='all',
                 base_temperature=0.5):
        super(SupConLoss, self).__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
        self.base_temperature = base_temperature

    def forward(self, features, labels=None, mask=None):
        """Compute loss for model. If both `labels` and `mask` are None,
        it degenerates to SimCLR unsupervised loss:
        https://arxiv.org/pdf/2002.05709.pdf
        Args:
            features: hidden vector of shape [bsz, n_views, ...].
            labels: ground truth of shape [bsz].
            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j
                has the same class as sample i. Can be asymmetric.
        Returns:
            A loss scalar.
        """
        device = (torch.device('cuda')
                  if features.is_cuda
                  else torch.device('cpu'))

        if len(features.shape) < 3:
            raise ValueError('`features` needs to be [bsz, n_views, ...],'
                             'at least 3 dimensions are required')
        if len(features.shape) > 3:
            features = features.view(features.shape[0], features.shape[1], -1)

        batch_size = features.shape[0]
        if labels is not None and mask is not None:
            raise ValueError('Cannot define both `labels` and `mask`')
        elif labels is None and mask is None:
            mask = torch.eye(batch_size, dtype=torch.float32).to(device)
        elif labels is not None:
            labels = labels.contiguous().view(-1, 1)
            if labels.shape[0] != batch_size:
                raise ValueError('Num of labels does not match num of features')
            mask = torch.eq(labels, labels.T).float().to(device)
        else:
            mask = mask.float().to(device)

        contrast_count = features.shape[1]
        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
        if self.contrast_mode == 'one':
            anchor_feature = features[:, 0]
            anchor_count = 1
        elif self.contrast_mode == 'all':
            anchor_feature = contrast_feature
            anchor_count = contrast_count
        else:
            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))

        # compute logits
        anchor_dot_contrast = torch.div(
            torch.matmul(anchor_feature, contrast_feature.T),
            self.temperature)
        # for numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        # tile mask
        mask = mask.repeat(anchor_count, contrast_count)
        # mask-out self-contrast cases
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask

        # compute log_prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        # compute mean of log-likelihood over positive
        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)

        # loss -> the gradient scales inversely with choice of temperature T ; therefore we rescale the loss by T during training for stability
    
        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos
        loss = loss.view(anchor_count, batch_size).mean()

        return loss


class SimLoss(nn.Module):
    def __init__(self, out_dim, ncrops, teacher_temp, student_temp=0.1):
        super().__init__()
        self.student_temp = student_temp
        self.ncrops = ncrops
        self.teacher_temp = teacher_temp

    def forward(self, student_output, teacher_output, epoch):
        """
        Cross-entropy between softmax outputs of the teacher and student networks.
        """
        student_out = student_output / self.student_temp
        student_out = student_out.chunk(self.ncrops, dim = 1)

        # teacher centering and sharpening
        teacher_out = F.softmax((teacher_output) / self.teacher_temp, dim=-1)
        teacher_out = teacher_out.detach().chunk(2, dim = 1)

        total_loss = 0
        n_loss_terms = 0
        for iq, q in enumerate(teacher_out):
            for v in range(len(student_out)):
                if v == iq:
                    # we skip cases where student and teacher operate on the same view
                    continue
                loss = torch.sum(-q * F.log_softmax(student_out[v], dim=-1), dim=-1)
                total_loss += loss.mean()
                n_loss_terms += 1
        total_loss /= n_loss_terms
        self.update_center(teacher_output)
        return total_loss


def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):
    warmup_schedule = np.array([])
    warmup_iters = warmup_epochs * niter_per_ep
    if warmup_epochs > 0:
        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)

    iters = np.arange(epochs * niter_per_ep - warmup_iters)
    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))

    schedule = np.concatenate((warmup_schedule, schedule))
    assert len(schedule) == epochs * niter_per_ep
    return schedule

class distLinear(nn.Module):
    def __init__(self, 
                indim: int, 
                outdim: int,
                ):
        super(distLinear, self).__init__()
        self.fc = nn.Linear(indim, outdim, bias = False)
        self.class_wise_learnable_norm = True  
        if self.class_wise_learnable_norm:      
            WeightNorm.apply(self.fc, 'weight', dim=0) 
            #split the weight update component to direction and norm      

        self.scale_factor = 2
        #a fixed scale factor to scale the output of cos value into a reasonably large input for softmax, for to reproduce the result of CUB with ResNet10, use 4.  

    def forward(self, x):
        x_norm = torch.norm(x, p=2, dim =1).unsqueeze(1).expand_as(x)
        x_normalized = x.div(x_norm + 0.00001)
        if not self.class_wise_learnable_norm:
            L_norm = torch.norm(self.fc.weight.data, p=2, dim =1).unsqueeze(1).expand_as(self.fc.weight.data)
            self.fc.weight.data = self.fc.weight.data.div(L_norm + 0.00001)
        cos_dist = self.fc(x_normalized) 
        #matrix product by forward function, but when using WeightNorm, this also multiply the cosine distance by a class-wise learnable norm
        scores = self.scale_factor* (cos_dist) 

        return scores


### Self-distillation functions adopted from https://github.com/WangYueFt/rfs arXiv:2003.11539 ###

class DistillKL(nn.Module):
    """KL divergence for distillation"""
    def __init__(self, T):
        super(DistillKL, self).__init__()
        self.T = T

    def forward(self, y_s, y_t):
        p_s = F.log_softmax(y_s/self.T, dim=1) # log_softmax
        p_t = F.softmax(y_t/self.T, dim=1)
        loss = F.kl_div(p_s, p_t, size_average=False) * (self.T**2) / y_s.shape[0]
        return loss


class NCESoftmax(nn.Module):

    def __init__(self, inputSize, outputSize, K, T=0.07, momentum=0.5):
        super(NCESoftmax, self).__init__()
        self.nLem = outputSize
        self.unigrams = torch.ones(self.nLem)
        self.multinomial = AliasMethod(self.unigrams)
        self.multinomial.cuda()
        self.K = K

        self.register_buffer('params', torch.tensor([K, T, -1, -1, momentum]))
        stdv = 1. / math.sqrt(inputSize / 3)
        self.register_buffer('memory_l', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))
        self.register_buffer('memory_ab', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))

    def forward(self, l, ab, y, idx=None):
        K = int(self.params[0].item())
        T = self.params[1].item()
        Z_l = self.params[2].item()
        Z_ab = self.params[3].item()

        momentum = self.params[4].item()
        batchSize = l.size(0)
        outputSize = self.memory_l.size(0)
        inputSize = self.memory_l.size(1)

        # original score computation
        if idx is None:
            idx = self.multinomial.draw(batchSize * (self.K + 1)).view(batchSize, -1)
            idx.select(1, 0).copy_(y.data)
        # sample
        weight_l = torch.index_select(self.memory_l, 0, idx.view(-1)).detach()
        weight_l = weight_l.view(batchSize, K + 1, inputSize)
        out_ab = torch.bmm(weight_l, ab.view(batchSize, inputSize, 1))
        # out_ab = torch.exp(torch.div(out_ab, T))
        out_ab = torch.div(out_ab, T)
        # sample
        weight_ab = torch.index_select(self.memory_ab, 0, idx.view(-1)).detach()
        weight_ab = weight_ab.view(batchSize, K + 1, inputSize)
        out_l = torch.bmm(weight_ab, l.view(batchSize, inputSize, 1))
        # out_l = torch.exp(torch.div(out_l, T))
        out_l = torch.div(out_l, T)

        # set Z if haven't been set yet
        if Z_l < 0:
            # self.params[2] = out_l.mean() * outputSize
            self.params[2] = 1
            Z_l = self.params[2].clone().detach().item()
            print("normalization constant Z_l is set to {:.1f}".format(Z_l))
        if Z_ab < 0:
            # self.params[3] = out_ab.mean() * outputSize
            self.params[3] = 1
            Z_ab = self.params[3].clone().detach().item()
            print("normalization constant Z_ab is set to {:.1f}".format(Z_ab))

        # compute out_l, out_ab
        # out_l = torch.div(out_l, Z_l).contiguous()
        # out_ab = torch.div(out_ab, Z_ab).contiguous()
        out_l = out_l.contiguous()
        out_ab = out_ab.contiguous()

        # update memory
        with torch.no_grad():
            l_pos = torch.index_select(self.memory_l, 0, y.view(-1))
            l_pos.mul_(momentum)
            l_pos.add_(torch.mul(l, 1 - momentum))
            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_l = l_pos.div(l_norm)
            self.memory_l.index_copy_(0, y, updated_l)

            ab_pos = torch.index_select(self.memory_ab, 0, y.view(-1))
            ab_pos.mul_(momentum)
            ab_pos.add_(torch.mul(ab, 1 - momentum))
            ab_norm = ab_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_ab = ab_pos.div(ab_norm)
            self.memory_ab.index_copy_(0, y, updated_ab)

        return out_l, out_ab


class NCEAverage(nn.Module):

    def __init__(self, inputSize, outputSize, K, T=0.07, momentum=0.5):
        super(NCEAverage, self).__init__()
        self.nLem = outputSize
        self.unigrams = torch.ones(self.nLem)
        self.multinomial = AliasMethod(self.unigrams)
        self.multinomial.cuda()
        self.K = K

        self.register_buffer('params', torch.tensor([K, T, -1, -1, momentum]))
        stdv = 1. / math.sqrt(inputSize / 3)
        self.register_buffer('memory_l', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))
        self.register_buffer('memory_ab', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))

    def forward(self, l, ab, y, idx=None):
        K = int(self.params[0].item())
        T = self.params[1].item()
        Z_l = self.params[2].item()
        Z_ab = self.params[3].item()

        momentum = self.params[4].item()
        batchSize = l.size(0)
        outputSize = self.memory_l.size(0)
        inputSize = self.memory_l.size(1)

        # original score computation
        if idx is None:
            idx = self.multinomial.draw(batchSize * (self.K + 1)).view(batchSize, -1)
            idx.select(1, 0).copy_(y.data)
        # sample
        weight_l = torch.index_select(self.memory_l, 0, idx.view(-1)).detach()
        weight_l = weight_l.view(batchSize, K + 1, inputSize)
        out_ab = torch.bmm(weight_l, ab.view(batchSize, inputSize, 1))
        out_ab = torch.exp(torch.div(out_ab, T))
        # sample
        weight_ab = torch.index_select(self.memory_ab, 0, idx.view(-1)).detach()
        weight_ab = weight_ab.view(batchSize, K + 1, inputSize)
        out_l = torch.bmm(weight_ab, l.view(batchSize, inputSize, 1))
        out_l = torch.exp(torch.div(out_l, T))

        # set Z if haven't been set yet
        if Z_l < 0:
            self.params[2] = out_l.mean() * outputSize
            Z_l = self.params[2].clone().detach().item()
            print("normalization constant Z_l is set to {:.1f}".format(Z_l))
        if Z_ab < 0:
            self.params[3] = out_ab.mean() * outputSize
            Z_ab = self.params[3].clone().detach().item()
            print("normalization constant Z_ab is set to {:.1f}".format(Z_ab))

        # compute out_l, out_ab
        out_l = torch.div(out_l, Z_l).contiguous()
        out_ab = torch.div(out_ab, Z_ab).contiguous()

        # update memory
        with torch.no_grad():
            l_pos = torch.index_select(self.memory_l, 0, y.view(-1))
            l_pos.mul_(momentum)
            l_pos.add_(torch.mul(l, 1 - momentum))
            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_l = l_pos.div(l_norm)
            self.memory_l.index_copy_(0, y, updated_l)

            ab_pos = torch.index_select(self.memory_ab, 0, y.view(-1))
            ab_pos.mul_(momentum)
            ab_pos.add_(torch.mul(ab, 1 - momentum))
            ab_norm = ab_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_ab = ab_pos.div(ab_norm)
            self.memory_ab.index_copy_(0, y, updated_ab)

        return out_l, out_ab


class NCEAverageWithZ(nn.Module):

    def __init__(self, inputSize, outputSize, K, T=0.07, momentum=0.5, z=None):
        super(NCEAverageWithZ, self).__init__()
        self.nLem = outputSize
        self.unigrams = torch.ones(self.nLem)
        self.multinomial = AliasMethod(self.unigrams)
        self.multinomial.cuda()
        self.K = K

        if z is None or z <= 0:
            z = -1
        else:
            pass
        self.register_buffer('params', torch.tensor([K, T, z, z, momentum]))
        stdv = 1. / math.sqrt(inputSize / 3)
        self.register_buffer('memory_l', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))
        self.register_buffer('memory_ab', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))

    def forward(self, l, ab, y, idx=None):
        K = int(self.params[0].item())
        T = self.params[1].item()
        Z_l = self.params[2].item()
        Z_ab = self.params[3].item()

        momentum = self.params[4].item()
        batchSize = l.size(0)
        outputSize = self.memory_l.size(0)
        inputSize = self.memory_l.size(1)

        # original score computation
        if idx is None:
            idx = self.multinomial.draw(batchSize * (self.K + 1)).view(batchSize, -1)
            idx.select(1, 0).copy_(y.data)
        # sample
        weight_l = torch.index_select(self.memory_l, 0, idx.view(-1)).detach()
        weight_l = weight_l.view(batchSize, K + 1, inputSize)
        out_ab = torch.bmm(weight_l, ab.view(batchSize, inputSize, 1))
        out_ab = torch.exp(torch.div(out_ab, T))
        # sample
        weight_ab = torch.index_select(self.memory_ab, 0, idx.view(-1)).detach()
        weight_ab = weight_ab.view(batchSize, K + 1, inputSize)
        out_l = torch.bmm(weight_ab, l.view(batchSize, inputSize, 1))
        out_l = torch.exp(torch.div(out_l, T))

        # set Z if haven't been set yet
        if Z_l < 0:
            self.params[2] = out_l.mean() * outputSize
            Z_l = self.params[2].clone().detach().item()
            print("normalization constant Z_l is set to {:.1f}".format(Z_l))
        if Z_ab < 0:
            self.params[3] = out_ab.mean() * outputSize
            Z_ab = self.params[3].clone().detach().item()
            print("normalization constant Z_ab is set to {:.1f}".format(Z_ab))

        # compute out_l, out_ab
        out_l = torch.div(out_l, Z_l).contiguous()
        out_ab = torch.div(out_ab, Z_ab).contiguous()

        # update memory
        with torch.no_grad():
            l_pos = torch.index_select(self.memory_l, 0, y.view(-1))
            l_pos.mul_(momentum)
            l_pos.add_(torch.mul(l, 1 - momentum))
            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_l = l_pos.div(l_norm)
            self.memory_l.index_copy_(0, y, updated_l)

            ab_pos = torch.index_select(self.memory_ab, 0, y.view(-1))
            ab_pos.mul_(momentum)
            ab_pos.add_(torch.mul(ab, 1 - momentum))
            ab_norm = ab_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_ab = ab_pos.div(ab_norm)
            self.memory_ab.index_copy_(0, y, updated_ab)

        return out_l, out_ab


class NCEAverageFull(nn.Module):

    def __init__(self, inputSize, outputSize, T=0.07, momentum=0.5):
        super(NCEAverageFull, self).__init__()
        self.nLem = outputSize

        self.register_buffer('params', torch.tensor([T, -1, -1, momentum]))
        stdv = 1. / math.sqrt(inputSize / 3)
        self.register_buffer('memory_l', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))
        self.register_buffer('memory_ab', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))

    def forward(self, l, ab, y):
        T = self.params[0].item()
        Z_l = self.params[1].item()
        Z_ab = self.params[2].item()

        momentum = self.params[3].item()
        batchSize = l.size(0)
        outputSize = self.memory_l.size(0)
        inputSize = self.memory_l.size(1)

        # score computation
        idx1 = y.unsqueeze(1).expand(-1, inputSize).unsqueeze(1).expand(-1, 1, -1)
        idx2 = torch.zeros(batchSize).long().cuda()
        idx2 = idx2.unsqueeze(1).expand(-1, inputSize).unsqueeze(1).expand(-1, 1, -1)
        # sample
        weight_l = self.memory_l.clone().detach().unsqueeze(0).expand(batchSize, outputSize, inputSize)
        weight_l_1 = weight_l.gather(dim=1, index=idx1)
        weight_l_2 = weight_l.gather(dim=1, index=idx2)
        weight_l.scatter_(1, idx1, weight_l_2)
        weight_l.scatter_(1, idx2, weight_l_1)
        out_ab = torch.bmm(weight_l, ab.view(batchSize, inputSize, 1))
        out_ab = torch.exp(torch.div(out_ab, T))
        # sample
        weight_ab = self.memory_ab.clone().detach().unsqueeze(0).expand(batchSize, outputSize, inputSize)
        weight_ab_1 = weight_ab.gather(dim=1, index=idx1)
        weight_ab_2 = weight_ab.gather(dim=1, index=idx2)
        weight_ab.scatter_(1, idx1, weight_ab_2)
        weight_ab.scatter_(1, idx2, weight_ab_1)
        out_l = torch.bmm(weight_ab, l.view(batchSize, inputSize, 1))
        out_l = torch.exp(torch.div(out_l, T))

        # set Z if haven't been set yet
        if Z_l < 0:
            self.params[1] = out_l.mean() * outputSize
            Z_l = self.params[1].clone().detach().item()
            print("normalization constant Z_l is set to {:.1f}".format(Z_l))
        if Z_ab < 0:
            self.params[2] = out_ab.mean() * outputSize
            Z_ab = self.params[2].clone().detach().item()
            print("normalization constant Z_ab is set to {:.1f}".format(Z_ab))

        # compute out_l, out_ab
        out_l = torch.div(out_l, Z_l).contiguous()
        out_ab = torch.div(out_ab, Z_ab).contiguous()

        # update memory
        with torch.no_grad():
            l_pos = torch.index_select(self.memory_l, 0, y.view(-1))
            l_pos.mul_(momentum)
            l_pos.add_(torch.mul(l, 1 - momentum))
            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_l = l_pos.div(l_norm)
            self.memory_l.index_copy_(0, y, updated_l)

            ab_pos = torch.index_select(self.memory_ab, 0, y.view(-1))
            ab_pos.mul_(momentum)
            ab_pos.add_(torch.mul(ab, 1 - momentum))
            ab_norm = ab_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_ab = ab_pos.div(ab_norm)
            self.memory_ab.index_copy_(0, y, updated_ab)

        return out_l, out_ab


class NCEAverageFullSoftmax(nn.Module):

    def __init__(self, inputSize, outputSize, T=1, momentum=0.5):
        super(NCEAverageFullSoftmax, self).__init__()
        self.nLem = outputSize

        self.register_buffer('params', torch.tensor([T, momentum]))
        stdv = 1. / math.sqrt(inputSize / 3)
        self.register_buffer('memory_l', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))
        self.register_buffer('memory_ab', torch.rand(outputSize, inputSize).mul_(2 * stdv).add_(-stdv))

    def forward(self, l, ab, y):
        T = self.params[0].item()
        momentum = self.params[1].item()
        batchSize = l.size(0)
        outputSize = self.memory_l.size(0)
        inputSize = self.memory_l.size(1)

        # score computation
        # weight_l = self.memory_l.unsqueeze(0).expand(batchSize, outputSize, inputSize).detach()
        weight_l = self.memory_l.clone().unsqueeze(0).expand(batchSize, outputSize, inputSize).detach()
        out_ab = torch.bmm(weight_l, ab.view(batchSize, inputSize, 1))
        out_ab = out_ab.div(T)
        out_ab = out_ab.squeeze().contiguous()

        # weight_ab = self.memory_ab.unsqueeze(0).expand(batchSize, outputSize, inputSize).detach()
        weight_ab = self.memory_ab.clone().unsqueeze(0).expand(batchSize, outputSize, inputSize).detach()
        out_l = torch.bmm(weight_ab, l.view(batchSize, inputSize, 1))
        out_l = out_l.div(T)
        out_l = out_l.squeeze().contiguous()

        # update memory
        with torch.no_grad():
            l_pos = torch.index_select(self.memory_l, 0, y.view(-1))
            l_pos.mul_(momentum)
            l_pos.add_(torch.mul(l, 1 - momentum))
            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_l = l_pos.div(l_norm)
            self.memory_l.index_copy_(0, y, updated_l)

            ab_pos = torch.index_select(self.memory_ab, 0, y.view(-1))
            ab_pos.mul_(momentum)
            ab_pos.add_(torch.mul(ab, 1 - momentum))
            ab_norm = ab_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_ab = ab_pos.div(ab_norm)
            self.memory_ab.index_copy_(0, y, updated_ab)

        return out_l, out_ab

    def update_memory(self, l, ab, y):
        momentum = self.params[1].item()
        # update memory
        with torch.no_grad():
            l_pos = torch.index_select(self.memory_l, 0, y.view(-1))
            l_pos.mul_(momentum)
            l_pos.add_(torch.mul(l, 1 - momentum))
            l_norm = l_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_l = l_pos.div(l_norm)
            self.memory_l.index_copy_(0, y, updated_l)

            ab_pos = torch.index_select(self.memory_ab, 0, y.view(-1))
            ab_pos.mul_(momentum)
            ab_pos.add_(torch.mul(ab, 1 - momentum))
            ab_norm = ab_pos.pow(2).sum(1, keepdim=True).pow(0.5)
            updated_ab = ab_pos.div(ab_norm)
            self.memory_ab.index_copy_(0, y, updated_ab)

class NCELoss(nn.Module):
    """NCE contrastive loss"""
    def __init__(self, opt, n_data):
        super(NCELoss, self).__init__()
        self.contrast = NCEAverage(opt.feat_dim, n_data, opt.nce_k, opt.nce_t, opt.nce_m)
        self.criterion_t = NCECriterion(n_data)
        self.criterion_s = NCECriterion(n_data)

    def forward(self, f_s, f_t, idx, contrast_idx=None):
        out_s, out_t = self.contrast(f_s, f_t, idx, contrast_idx)
        s_loss = self.criterion_s(out_s)
        t_loss = self.criterion_t(out_t)
        loss = s_loss + t_loss
        return loss


class NCESoftmaxLoss(nn.Module):
    """info NCE style loss, softmax"""
    def __init__(self, opt, n_data):
        super(NCESoftmaxLoss, self).__init__()
        self.contrast = NCESoftmax(opt.feat_dim, n_data, opt.nce_k, opt.nce_t, opt.nce_m)
        self.criterion_t = nn.CrossEntropyLoss()
        self.criterion_s = nn.CrossEntropyLoss()

    def forward(self, f_s, f_t, idx, contrast_idx=None):
        out_s, out_t = self.contrast(f_s, f_t, idx, contrast_idx)
        bsz = f_s.shape[0]
        label = torch.zeros([bsz, 1]).cuda().long()
        s_loss = self.criterion_s(out_s, label)
        t_loss = self.criterion_t(out_t, label)
        loss = s_loss + t_loss
        return loss


class Attention(nn.Module):
    """attention transfer loss"""
    def __init__(self, p=2):
        super(Attention, self).__init__()
        self.p = p

    def forward(self, g_s, g_t):
        return [self.at_loss(f_s, f_t) for f_s, f_t in zip(g_s, g_t)]

    def at_loss(self, f_s, f_t):
        s_H, t_H = f_s.shape[2], f_t.shape[2]
        if s_H > t_H:
            f_s = F.adaptive_avg_pool2d(f_s, (t_H, t_H))
        elif s_H < t_H:
            f_t = F.adaptive_avg_pool2d(f_t, (s_H, s_H))
        else:
            pass
        return (self.at(f_s) - self.at(f_t)).pow(2).mean()

    def at(self, f):
        return F.normalize(f.pow(self.p).mean(1).view(f.size(0), -1))


class HintLoss(nn.Module):
    """regression loss from hints"""
    def __init__(self):
        super(HintLoss, self).__init__()
        self.crit = nn.MSELoss()

    def forward(self, f_s, f_t):
        loss = self.crit(f_s, f_t)
        return loss
